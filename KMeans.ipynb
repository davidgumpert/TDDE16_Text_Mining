{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all needed packages for the full notebook. Only needs to be ran once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /opt/anaconda3/envs/TDDE16_Project/lib/python3.7/site-packages (1.0.0)\r\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/TDDE16_Project/lib/python3.7/site-packages (from rouge) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip as gz\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy as sp\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting local file path\n",
    "path = \"release/train.jsonl.gz\"\n",
    "\n",
    "# Creating list entity to hold full set of loaded data\n",
    "data = []\n",
    "\n",
    "# Using gz to set path to zip file and iteritavly load each json line\n",
    "with gz.open(path) as f:\n",
    "    for ln in f:\n",
    "        obj = json.loads(ln)\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting appropriate data and transforming to pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df_extractive = df[df.density_bin == 'extractive']\n",
    "\n",
    "# Filtering on rough estimate of lenght text\n",
    "article_lengths = [len(text.split()) for text in df_extractive.text]\n",
    "_ = plt.hist(article_lengths, bins = 100, range = (0, 2000))\n",
    "length_check = [len > 250 for len in article_lengths]\n",
    "df_extractive = df_extractive[length_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SPLITTING FUNCTION\n",
    "def sentence_splitting(doc):\n",
    "    article = nlp(doc['text'])\n",
    "    sentences = [sent.string.strip() for sent in article.sents]\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    all_tokens = []\n",
    "    for sentence in article.sents:\n",
    "        tokens = []\n",
    "        for token in sentence:\n",
    "            if token.is_stop == False and token.is_alpha == True:\n",
    "                t = token.lemma_.lower()\n",
    "                tokens.append(t)\n",
    "                all_tokens.append(t)\n",
    "        if len(tokens) > 0:\n",
    "            tokenized_sentences.append(tokens)  \n",
    "    \n",
    "    return sentences, tokenized_sentences, all_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE VECTOR REPRESENTATION FUNCTION\n",
    "def vectorize_sentences(tokenized_sentences, model):\n",
    "    sentence_vectors = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        vec = np.zeros(300)\n",
    "        for token in sentence:\n",
    "            vec = vec + model.wv[token]\n",
    "        vec = vec/len(sentence)\n",
    "        sentence_vectors.append(vec)\n",
    "    \n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence clustering with Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(k, sentence_vectors):\n",
    "    kmeans = KMeans(k, init = 'k-means++', random_state = 42)\n",
    "    kmeans_fit = kmeans.fit(sentence_vectors)\n",
    "    pred = kmeans_fit.predict(sentence_vectors)\n",
    "    \n",
    "    return kmeans_fit, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT PCA REDUCED VECTOR CLUSTER ASSIGNMENTS\n",
    "def kmeans_pca_plot(sentence_vectors, predictions):\n",
    "    k = len(set(predictions))\n",
    "    pca = PCA(n_components = 2)\n",
    "    pca_fit = sklearn_pca.fit_transform(sentence_vectors)\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(k): \n",
    "        cluster_predictions = pca_fit[predictions == i]\n",
    "        plt.scatter(cluster_predictions[:,0] , cluster_predictions[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentences closest to each cluster centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT REPRESENTATIVE CLUSTER SENTENCES\n",
    "def extract_sentences(kmeans_fit, sentence_vectors):\n",
    "    closest_indices, _ = pairwise_distances_argmin_min(kmeans_fit.cluster_centers_, sentence_vectors)\n",
    "    closest_indices = np.sort(closest_indices)\n",
    "    \n",
    "    summary = []\n",
    "    for index in closest_indices:\n",
    "        summary.append(sentences[index])\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score summaries with ROUGE and BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "# ROUGE AND BLEU SCORING FUNCTION\n",
    "def rouge_blue_scoring(summary, reference):\n",
    "    rouge_score = rouge.get_scores(summary, reference)\n",
    "    \n",
    "    return rouge_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_extractive[0:10]\n",
    "summaries = []\n",
    "rouge_scores = []\n",
    "\n",
    "for index, doc in corpus.iterrows():\n",
    "    sentences, tokenized_sentences, all_tokens = sentence_splitting(doc)\n",
    "    model = Word2Vec(tokenized_sentences, min_count=1,size= 300)\n",
    "    sentence_vectors = vectorize_sentences(tokenized_sentences, model)\n",
    "    k = 3\n",
    "    kmeans_fit, predictions = kmeans_clustering(k, sentence_vectors)\n",
    "    #kmeans_pca_plot(sentence_vectors, predictions)\n",
    "    summary = extract_sentences(kmeans_fit, sentence_vectors)\n",
    "    summaries.append(summary)\n",
    "    rouge_score= rouge_blue_scoring(summary, doc.summary)\n",
    "    rouge_scores.append(rouge_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
