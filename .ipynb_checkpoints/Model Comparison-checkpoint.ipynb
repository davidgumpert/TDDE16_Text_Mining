{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all needed packages for the full notebook. Only needs to be ran once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip as gz\n",
    "import json\n",
    "import sys as sklearn\n",
    "import spacy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rouge import Rouge\n",
    "from rouge_score import rouge_scorer\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA THINNING FUNCTION\n",
    "def data_thinning(data): \n",
    "    # Extracting appropriate data and transforming to pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    df_extractive = df[df.density_bin == 'extractive']\n",
    "\n",
    "    # Filtering on rough estimate of lenght text\n",
    "    article_lengths = [len(text.split()) for text in df_extractive.text]\n",
    "    _ = plt.hist(article_lengths, bins = 100, range = (0, 2000))\n",
    "    length_check = [len > 250 for len in article_lengths]\n",
    "    df_extractive = df_extractive[length_check]\n",
    "\n",
    "    # Inspecting head of dataframe for inspection\n",
    "    df_extractive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SPLITTING FUNCTION\n",
    "def sentence_splitting(doc):\n",
    "    doc = nlp_sentencizer(doc['text'])\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING FUNCTION\n",
    "def preprocess(text):\n",
    "    # Transform text with SpaCy model for NLP procedures\n",
    "    text = nlp(text)\n",
    "    \n",
    "    # loop through the words in the text, removing stopwords and numerics\n",
    "    # Assign the remaining tokens to the token list in the lemma form\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        if token.is_stop == False and token.is_alpha == True:\n",
    "            tokens.append(token.lemma_)\n",
    "    \n",
    "    # Return all lemmatized tokens in the input text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL','rougeLsum'], use_stemmer=True)\n",
    "# ROUGE AND BLEU SCORING FUNCTION\n",
    "def rouge_blue_scoring(summary, reference):\n",
    "    rouge_score = rouge.get_scores(summary, reference)\n",
    "    rouge_score_2 = scorer.score(summary, reference)\n",
    "    return rouge_score_2, rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse TF-IDF matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF MATRIX CREATION FUNCTION\n",
    "def create_tfidf_matrix(corpus, preprocessor = preprocess):\n",
    "    #Initializing ScikitLearn TF-IDF vectorizer and creating TF-IDF sparse matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer = preprocessor)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus['text'])\n",
    "    \n",
    "    # Saving list of all corpus tokens\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Inspecting dimension of sparse matrix\n",
    "    # Rows should equal no. of df_extractive data entities\n",
    "    # Number of columns equals number of unique corpus tokens\n",
    "    print(\"TF-IDF matrix dimension: \", tfidf_matrix.get_shape(), \"\\nAligning with no. df_extractive enteties? \", tfidf_matrix.get_shape()[0] == len(df_extractive))\n",
    "    \n",
    "    # Returning TF-IDF matrix\n",
    "    return tfidf_matrix, feature_names### Sparse TF-IDF matrix representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token and TF-IDF score pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKEN TFIDF PAIRING FUNCTION\n",
    "def token_tfidf_ranking(feature_names, tfidf_matrix, row_index = 0):\n",
    "    # Exctracting indices of document tokens from the TF-IDF matrix\n",
    "    token_indices = tfidf_matrix[row_index,:].nonzero()[1]\n",
    "   \n",
    "    # Extract token names and pair with corresponding TF-IDF value from the TF-IDF matrix\n",
    "    # Sort by TF-IDF score\n",
    "    token_tfidf = pd.DataFrame(np.column_stack(([feature_names[index] for index in token_indices], [tfidf_matrix[row_index, x] for x in token_indices])), columns=['token', 'tfidf_score'])\n",
    "    token_tfidf = token_tfidf.sort_values(by='tfidf_score', ascending=False)\n",
    "    \n",
    "    #token_tfidf = token_tfidf.astype({\"word\": str, \"tfidf_score\": float})\n",
    "    \n",
    "    # Return the sorted list of (token, TF-IDF value) data frame\n",
    "    return token_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading new model from SpaCy and adding sentencizer pipeline\n",
    "nlp_sentencizer = sp.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "nlp_sentencizer.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence level tokenization and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SCORING FUNTION\n",
    "def sentence_scoring(sentences, token_tfidf_pairs):\n",
    "    sentence_scores = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        score = 0\n",
    "        sentence_length = len(sentence)\n",
    "\n",
    "        # Using preprocessing function to extract sentence tokens\n",
    "        sentence_tokens = preprocess(sentence)\n",
    "\n",
    "        # Summation of sentence tokens' TF-IDF values \n",
    "        for token in sentence_tokens:\n",
    "            token = token.lower()\n",
    "            if token in token_tfidf_pairs['token'].values:\n",
    "                score = score + float(token_tfidf_pairs.loc[token_tfidf_pairs['token'] == token]['tfidf_score'].values)\n",
    "\n",
    "        # Normalizing sentence score dependent on sentence length\n",
    "        score = score / sentence_length\n",
    "        \n",
    "        # Append to list of sentence scores\n",
    "        sentence_scores.append((i,score))\n",
    "    \n",
    "    # Save scores in pd dataframe\n",
    "    sentence_scores = pd.DataFrame(sentence_scores,columns=[\"sentence_index\", \"sentence_score\"]).sort_values(by='sentence_score', ascending=False)\n",
    "    \n",
    "    # Return final sentence scores\n",
    "    return sentence_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE EXTRACTION FUNCTION\n",
    "def sentence_extraction(sentences, sentence_scores, n):\n",
    "    summary = []\n",
    "    \n",
    "    # Extracting indices of the n number of top scoring sentences\n",
    "    # sort them in ascending order\n",
    "    top_sentence_indices = np.sort(sentence_scores[0:n]['sentence_index'].values)\n",
    "    \n",
    "    # Extracting the original sentences and appending to summary list\n",
    "    for index in top_sentence_indices:\n",
    "        summary.append(sentences[index])\n",
    "        \n",
    "    # Joining summary sentences\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFISF Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse TF-ISF matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-ISF MATRIX CREATION FUNCTION\n",
    "def create_tfisf_matrix(sentences, preprocessor = preprocess):\n",
    "    #Initializing ScikitLearn TF-IDF vectorizer and creating TF-IDF sparse matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer = preprocessor)\n",
    "    tfisf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Saving list of all corpus tokens\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Returning TF-IDF matrix\n",
    "    return tfisf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SCORING FUNCTION\n",
    "def tfisf_sentence_scoring(tfisf_matrix, feature_names, sentences):\n",
    "    sentence_scores = []\n",
    "    tfisf_matrix =  pd.DataFrame.sparse.from_spmatrix(tfisf_matrix, columns=feature_names)\n",
    "    tfisf_matrix_row_sum = tfisf_matrix.sum(axis = 1)\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_scores.append((i, tfisf_matrix_row_sum[i]/len(sentence)))\n",
    "    \n",
    "    sentence_scores = pd.DataFrame(sentence_scores,columns=[\"sentence_index\", \"sentence_score\"]).sort_values(by='sentence_score', ascending=False)\n",
    "    \n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract summary sentences and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE EXTRACTION FUNCTION\n",
    "def tfisf_sentence_extraction(sentences, sentence_scores, n):\n",
    "    summary = []\n",
    "    \n",
    "    # Extracting indices of the n number of top scoring sentences\n",
    "    # sort them in ascending order\n",
    "    top_sentence_indices = np.sort(sentence_scores[0:n].sentence_index.values)\n",
    "\n",
    "    # Extracting the original sentences and appending to summary list\n",
    "    for index in top_sentence_indices:\n",
    "        summary.append(sentences[index])\n",
    "        \n",
    "    # Joining summary sentences\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMILARITY MATRIX FUNCTIUON\n",
    "def create_sim_matrix(sentences, tfisf_matrix):\n",
    "    # Remove possible NaN or inf or -inf and replace with numerical value\n",
    "    tfisf_matrix = np.nan_to_num(tfisf_matrix)\n",
    "    \n",
    "    # Transform matrix to list representation, needed for the cosine distance function\n",
    "    tfisf_matrix_list = tfisf_matrix.toarray().tolist()\n",
    "    \n",
    "    # Initialize empty quadratic sentence similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    # Loop over all similarity matrix positions, calculating the cosine similarity sentence i,j\n",
    "    for i in range(len(sentences)):\n",
    "        # Ignore if sentence has no token, avoids cosine distance calculation of zero-vector\n",
    "        if sum(tfisf_matrix_list[i]) == 0:\n",
    "            continue\n",
    "        for j in range(len(sentences)):\n",
    "            # Ignore if both are same sentences or sentence has no token\n",
    "            if i == j or sum(tfisf_matrix_list[j]) == 0: \n",
    "                continue \n",
    "            similarity_matrix[i][j] = 1 - cosine_distance(tfisf_matrix_list[i], tfisf_matrix_list[j])\n",
    "    \n",
    "    # Return the final similarity matrix\n",
    "    return similarity_matrix\n",
    "## Similarity matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph representation and sentence scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SCORING FUNCTION\n",
    "def textrank_sentence_scoring(similarity_matrix, sentences):\n",
    "    # Create graphical representation from similarity matrix\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    \n",
    "    # Rank all sentences according to pagerank algorithm\n",
    "    sentence_scores = nx.pagerank(graph, max_iter = 500)\n",
    "    \n",
    "    # Sort all sentences and sentence index according to score\n",
    "    sentence_scores_sorted = sorted(((sentence_scores[i],i) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    \n",
    "    # Return the sorted sentence scores + index \n",
    "    return sentence_scores_sortedb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_sentence_extraction(sentences, sentence_scores, n):\n",
    "    summary = []\n",
    "    \n",
    "    # Sort sentence indices in ascending order\n",
    "    sentence_indices_sorted = sorted(sentence_scores[0:n], key=lambda tup: tup[1])\n",
    "    \n",
    "    # Extract sentences and append to summary in taht order\n",
    "    for i in sentence_indices_sorted:\n",
    "        summary.append(sentences[i[1]])\n",
    "    \n",
    "    # Join all sentences to form final summary\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    # Return summary\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SPLITTING FUNCTION\n",
    "def sentence_splitting(doc):\n",
    "    article = nlp(doc['text'])\n",
    "    sentences = [sent.string.strip() for sent in article.sents]\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    all_tokens = []\n",
    "    for sentence in article.sents:\n",
    "        tokens = []\n",
    "        for token in sentence:\n",
    "            if token.is_stop == False and token.is_alpha == True:\n",
    "                t = token.lemma_.lower()\n",
    "                tokens.append(t)\n",
    "                all_tokens.append(t)\n",
    "        if len(tokens) > 0:\n",
    "            tokenized_sentences.append(tokens)  \n",
    "    \n",
    "    return sentences, tokenized_sentences, all_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE VECTOR REPRESENTATION FUNCTION\n",
    "def vectorize_sentences(tokenized_sentences, model):\n",
    "    sentence_vectors = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        vec = np.zeros(300)\n",
    "        for token in sentence:\n",
    "            vec = vec + model.wv[token]\n",
    "        vec = vec/len(sentence)\n",
    "        sentence_vectors.append(vec)\n",
    "    \n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence clustering with Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(k, sentence_vectors):\n",
    "    kmeans = KMeans(k, init = 'k-means++', random_state = 42)\n",
    "    kmeans_fit = kmeans.fit(sentence_vectors)\n",
    "    pred = kmeans_fit.predict(sentence_vectors)\n",
    "    \n",
    "    return kmeans_fit, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT PCA REDUCED VECTOR CLUSTER ASSIGNMENTS\n",
    "def kmeans_pca_plot(sentence_vectors, predictions):\n",
    "    k = len(set(predictions))\n",
    "    pca = PCA(n_components = 2)\n",
    "    pca_fit = sklearn_pca.fit_transform(sentence_vectors)\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(k): \n",
    "        cluster_predictions = pca_fit[predictions == i]\n",
    "        plt.scatter(cluster_predictions[:,0] , cluster_predictions[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentences closest to each cluster centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT REPRESENTATIVE CLUSTER SENTENCES\n",
    "def extract_sentences(kmeans_fit, sentence_vectors):\n",
    "    closest_indices, _ = pairwise_distances_argmin_min(kmeans_fit.cluster_centers_, sentence_vectors)\n",
    "    closest_indices = np.sort(closest_indices)\n",
    "    \n",
    "    summary = []\n",
    "    for index in closest_indices:\n",
    "        summary.append(sentences[index])\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting local file path\n",
    "path = \"release/train.jsonl.gz\"\n",
    "\n",
    "# Creating list entity to hold full set of loaded data\n",
    "data = []\n",
    "\n",
    "# Using gz to set path to zip file and iteritavly load each json line\n",
    "with gz.open(path) as f:\n",
    "    for ln in f:\n",
    "        obj = json.loads(ln)\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting appropriate data and transforming to pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df_extractive = df[df.density_bin == 'extractive']\n",
    "\n",
    "# Filtering on rough estimate of lenght text\n",
    "article_lengths = [len(text.split()) for text in df_extractive.text]\n",
    "_ = plt.hist(article_lengths, bins = 100, range = (0, 2000))\n",
    "length_check = [len > 250 for len in article_lengths]\n",
    "df_extractive = df_extractive[length_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
