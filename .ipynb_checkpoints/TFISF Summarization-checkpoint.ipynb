{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all needed packages for the full notebook. Only needs to be ran once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip as gz\n",
    "import json\n",
    "import sys as sklearn\n",
    "import spacy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting local file path\n",
    "path = \"release/train.jsonl.gz\"\n",
    "\n",
    "# Creating list entity to hold full set of loaded data\n",
    "data = []\n",
    "\n",
    "# Using gz to set path to zip file and iteritavly load each json line\n",
    "with gz.open(path) as f:\n",
    "    for ln in f:\n",
    "        obj = json.loads(ln)\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting appropriate data and transforming to pandas dataframe\n",
    "df = pd.DataFrame(data[0:100000])\n",
    "df_extractive = df[df.density_bin == 'extractive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermidiate Input Representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SPLITTING FUNCTION\n",
    "def sentence_splitting(doc):\n",
    "    doc = nlp(doc['text'])\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse TF-ISF matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS FUNCTION \n",
    "def preprocess(text):\n",
    "    text = nlp(text)\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        if token.is_stop == False and token.is_alpha == True:\n",
    "            tokens.append(token.lemma_)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-ISF MATRIX CREATION FUNCTION\n",
    "def create_tfisf_matrix(sentences, preprocessor = preprocess):\n",
    "    #Initializing ScikitLearn TF-IDF vectorizer and creating TF-IDF sparse matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer = preprocessor)\n",
    "    tfisf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Saving list of all corpus tokens\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Inspecting dimension of sparse matrix\n",
    "    # Rows should equal no. of df_extractive data entities\n",
    "    # Number of columns equals number of unique corpus tokens\n",
    "    #print(\"TF-IDF matrix dimension: \", tfisf_matrix.get_shape(), \"\\nAligning with no. df_extractive enteties? \", tfisf_matrix.get_shape()[0] == len(sentences))\n",
    "    \n",
    "    # Returning TF-IDF matrix\n",
    "    return tfisf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SCORING FUNCTION\n",
    "def tfisf_sentence_scoring(tfisf_matrix, feature_names, sentences):\n",
    "    sentence_scores = []\n",
    "    tfisf_matrix =  pd.DataFrame.sparse.from_spmatrix(tfisf_matrix, columns=feature_names)\n",
    "    tfisf_matrix_row_sum = tfisf_matrix.sum(axis = 1)\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_scores.append((i, tfisf_matrix_row_sum[i]/len(sentence)))\n",
    "    \n",
    "    sentence_scores = pd.DataFrame(sentence_scores,columns=[\"sentence_index\", \"sentence_score\"]).sort_values(by='sentence_score', ascending=False)\n",
    "    \n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract summary sentences and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE EXTRACTION FUNCTION\n",
    "def tfisf_sentence_extraction(sentences, sentence_scores, n):\n",
    "    summary = []\n",
    "    \n",
    "    # Extracting indices of the n number of top scoring sentences\n",
    "    # sort them in ascending order\n",
    "    top_sentence_indices = np.sort(sentence_scores[0:n].sentence_index.values)\n",
    "\n",
    "    # Extracting the original sentences and appending to summary list\n",
    "    for index in top_sentence_indices:\n",
    "        summary.append(sentences[index])\n",
    "        \n",
    "    # Joining summary sentences\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Expiriment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_extractive[0:10]\n",
    "summaries = []\n",
    "\n",
    "for index, doc in corpus.iterrows():\n",
    "    sentences = sentence_splitting(doc)\n",
    "    tfisf_matrix, feature_names = create_tfisf_matrix(sentences = sentences)\n",
    "    sentence_scores = tfisf_sentence_scoring(tfisf_matrix, feature_names, sentences)\n",
    "    summary = tfisf_sentence_extraction(sentences, sentence_scores, 3)\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If the board rejects the resignation offer, it will publicly state why. The thing has gotten so out of hand that words almost fail me. The shareholders should not tolerate it.\"',\n",
       " 'Oh, we\\'ve bee n there. I like quiet.\" Spooky.',\n",
       " 'In a sautÃ© pan, heat the olive oil over low heat. SautÃ© the spinach and onion for 10 minutes. Mix in the cheese, salt and pepper.',\n",
       " 'All day, every day, Cheryl Bernstein thanks her 16-month-old son. Bernstein is now a travel agent, and Flaumenbaum is studying desktop publishing. Yesterday, Baker held their seventh child, 11-month-old Ashley.',\n",
       " 'Area is 1 square mile. Population about 60,000. Covers 6 square miles.',\n",
       " 'The long arm of the tough new anti-smoking law stops at the gates of Rikers Island. Meringolo asked. he said. \"',\n",
       " 'Gag us with a spoon. Don\\'t tell anybody. Somebody\\'s got to eat, right?\"',\n",
       " \"It's light, summery and wonderful for sipping outside. Strain into a martini glass. Garnish with pink cotton candy all around the rim and serve.\",\n",
       " 'Some doubted the archbishop\\'s story yesterday. \"It was a waste of time,\" Koch said. \" She was as pure as driven snow, before and after.\"',\n",
       " '\"We shook hands. dead silence. I felt really bad,\" Kris Jenner said.']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
