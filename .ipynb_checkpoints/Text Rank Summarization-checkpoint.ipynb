{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All packages that are needed are imported in the next cell, should only be ran once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip as gz\n",
    "import json\n",
    "import sys as sklearn\n",
    "import spacy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"release/train.jsonl.gz\"\n",
    "data = []\n",
    "\n",
    "with gz.open(path) as f:\n",
    "    for ln in f:\n",
    "        obj = json.loads(ln)\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting appropriate data and transforming to pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df_extractive = df[df.density_bin == 'extractive']\n",
    "\n",
    "# Filtering on rough estimate of lenght text\n",
    "article_lengths = [len(text.split()) for text in df_extractive.text]\n",
    "_ = plt.hist(article_lengths, bins = 100, range = (0, 2000))\n",
    "length_check = [len > 250 for len in article_lengths]\n",
    "df_extractive = df_extractive[length_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'archive', 'title', 'date', 'text', 'summary', 'compression',\n",
       "       'coverage', 'density', 'compression_bin', 'coverage_bin',\n",
       "       'density_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extractive.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermidiate Input Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SPLITTING FUNCTION\n",
    "def sentence_splitting(doc):\n",
    "    # Using the NLP sentencizer pipeline to extract all text sentences\n",
    "    doc = nlp(doc['text'])\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse TF-ISF matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING FUNCTION\n",
    "def preprocess(text):\n",
    "    # Transform text with SpaCy model for NLP procedures\n",
    "    text = nlp(text)\n",
    "    \n",
    "    # loop through the words in the text, removing stopwords and numerics\n",
    "    # Assign the remaining tokens to the token list in the lemma form\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        if token.is_stop == False and token.is_alpha == True:\n",
    "            tokens.append(token.lemma_)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-ISF MATRIX CREATION FUNCTION\n",
    "def create_tfisf_matrix(sentences, preprocessor = preprocess):\n",
    "    #Initializing ScikitLearn TF-IDF vectorizer and creating TF-IDF sparse matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer = preprocessor)\n",
    "    tfisf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Saving list of all corpus tokens\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Returning TF-IDF matrix\n",
    "    return tfisf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMILARITY MATRIX FUNCTIUON\n",
    "def create_sim_matrix(sentences, tfisf_matrix):\n",
    "    # Remove possible NaN or inf or -inf and replace with numerical value\n",
    "    tfisf_matrix = np.nan_to_num(tfisf_matrix)\n",
    "    \n",
    "    # Transform matrix to list representation, needed for the cosine distance function\n",
    "    tfisf_matrix_list = tfisf_matrix.toarray().tolist()\n",
    "    \n",
    "    # Initialize empty quadratic sentence similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    # Loop over all similarity matrix positions, calculating the cosine similarity sentence i,j\n",
    "    for i in range(len(sentences)):\n",
    "        # Ignore if sentence has no token, avoids cosine distance calculation of zero-vector\n",
    "        if sum(tfisf_matrix_list[i]) == 0:\n",
    "            continue\n",
    "        for j in range(len(sentences)):\n",
    "            # Ignore if both are same sentences or sentence has no token\n",
    "            if i == j or sum(tfisf_matrix_list[j]) == 0: \n",
    "                continue \n",
    "            similarity_matrix[i][j] = 1 - cosine_distance(tfisf_matrix_list[i], tfisf_matrix_list[j])\n",
    "    \n",
    "    # Return the final similarity matrix\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph representation and sentence scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SCORING FUNCTION\n",
    "def textrank_sentence_scoring(similarity_matrix, sentences):\n",
    "    # Create graphical representation from similarity matrix\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    \n",
    "    # Rank all sentences according to pagerank algorithm\n",
    "    sentence_scores = nx.pagerank(graph, max_iter = 500)\n",
    "    \n",
    "    # Sort all sentences and sentence index according to score\n",
    "    sentence_scores_sorted = sorted(((sentence_scores[i],i) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    \n",
    "    # Return the sorted sentence scores + index \n",
    "    return sentence_scores_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_sentence_extraction(sentences, sentence_scores, n):\n",
    "    summary = []\n",
    "    \n",
    "    # Sort sentence indices in ascending order\n",
    "    sentence_indices_sorted = sorted(sentence_scores[0:n], key=lambda tup: tup[1])\n",
    "    \n",
    "    # Extract sentences and append to summary in taht order\n",
    "    for i in sentence_indices_sorted:\n",
    "        summary.append(sentences[i[1]])\n",
    "    \n",
    "    # Join all sentences to form final summary\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    # Return summary\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Running the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting corpus size to evaluate\n",
    "corpus = df_extractive[0:1000]\n",
    "summaries = []\n",
    "\n",
    "# Looping over every document in every corpus, running each model step\n",
    "for index, doc in corpus.iterrows():\n",
    "    # Sentence split document\n",
    "    sentences = sentence_splitting(doc)\n",
    "    \n",
    "    # TF-ISF matrix construction\n",
    "    tfisf_matrix, feature_names = create_tfisf_matrix(sentences = sentences)\n",
    "    \n",
    "    # Similarity matrix construction\n",
    "    similarity_matrix = create_sim_matrix(sentences, tfisf_matrix)\n",
    "    \n",
    "    # Scoring each document sentence\n",
    "    sentence_scores = textrank_sentence_scoring(similarity_matrix, sentences)\n",
    "    \n",
    "    # Extracting and merging sentences \n",
    "    summary = textrank_sentence_extraction(sentences, sentence_scores, 3)\n",
    "    \n",
    "    # Append summary to list of summaries \n",
    "    summaries.append(summary)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Manhattan prosecutors charged yesterday that Douglas Meyer, the former vice president of marketing at Syms Advertising, a subsidiary, wove a scam that fleeced his bosses for $5.5 million. Meyer pulled the worsted wool over their eyes, prosecutors said, by saying that he needed to hire three separate vendors to design, produce and place print advertising for the clothing firm. Prosecutors said that when the scam began in January 1998, Meyer and Jelle (Jay) Eijpe, 39, also of Secaucus, shared half of the money paid to Birnbach's allegedly fake firms.\""
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
